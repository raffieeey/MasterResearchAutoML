{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark results reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequirements\n",
    "This notebook requires a kernel running Python 3.5+.\n",
    "You can skip this section if the kernel is already configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "#!pip install jupyter_contrib_nbextensions\n",
    "#!jupyter contrib nbextension install --user\n",
    "#!jupyter nbextension enable python-markdown/main\n",
    "#!pip install jupyter_nbextensions_configurator\n",
    "#!jupyter nbextensions_configurator enable --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and selection of the results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython import display as idisplay\n",
    "import functools as ft\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import scipy as sp\n",
    "import seaborn as sb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#disabling this cell\n",
    "results_dir = \"./reports\"\n",
    "print(\"current working dir: {}\".format(os.getcwd()))\n",
    "try:\n",
    "    os.chdir(results_dir)\n",
    "except:\n",
    "    pass\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nfolds = 10\n",
    "ff = '%.6g'\n",
    "colormap = 'tab10'\n",
    "# colormap = 'Set2'\n",
    "# colormap = 'Dark2'\n",
    "\n",
    "renamings = dict(\n",
    "    constantpredictor_enc='constantpredictor'\n",
    ")\n",
    "excluded_frameworks = ['oboe']\n",
    "binary_score_label = 'AUC'\n",
    "multiclass_score_label = 'logloss'\n",
    "\n",
    "# impute_missing_with = 'constantpredictor'\n",
    "impute_missing_with = 'randomforest'\n",
    "zero_one_refs = ('constantpredictor', 'tunedrandomforest')\n",
    "\n",
    "all_results_files = {\n",
    "    'old': [\n",
    "        \"results_valid_ref.csv\", \"results_valid.csv\",\n",
    "        \"results_small-2c1h_ref.csv\", \"results_small-2c1h.csv\",\n",
    "        \"results_medium-4c1h_ref.csv\", \"results_medium-4c1h.csv\",\n",
    "        \"results_medium-4c4h_ref.csv\", \"results_medium-4c4h.csv\",\n",
    "    ],\n",
    "    '1h': [\n",
    "        \"results_small-8c1h_ref.csv\", \"results_small-8c1h.csv\",\n",
    "        \"results_medium-8c1h_ref.csv\", \"results_medium-8c1h.csv\",            \n",
    "    ],\n",
    "    '4h': [\n",
    "        \"results_small-8c4h_ref.csv\", \"results_small-8c4h.csv\",\n",
    "        \"results_medium-8c4h_ref.csv\", \"results_medium-8c4h.csv\",    \n",
    "        \"results_large-8c4h_ref.csv\", \"results_large-8c4h.csv\",       \n",
    "    ],\n",
    "    '8h': [\n",
    "        \"results_large-8c8h_ref.csv\", \"results_large-8c8h.csv\",        \n",
    "    ]\n",
    "}\n",
    "\n",
    "results_group = '4h'\n",
    "results_files = all_results_files[results_group]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading results, formatting and adding columns\n",
    "- `result` is the raw result metric computed from predictions at the end the benchmark.\n",
    "    For classification problems, it is usually `auc` for binomial classification and `logloss` for multinomial classification.\n",
    "- `score` ensures a standard comparison between tasks: **higher is always better**.\n",
    "- `norm_score` is a normalization of `score` on a `[0, 1]` scale, with `{{zero_one_refs[0]}}` score as `0` and `{{zero_one_refs[1]}}` score as `1`.\n",
    "- `imp_result` and `imp_score` for imputed results/scores. Given a task and a framework:\n",
    "    - if **all folds results/scores are missing**, then no imputation occurs, and the result is `nan` for each fold.\n",
    "    - if **only some folds results/scores are missing**, then the missing result is imputed by the `{{impute_missing_with}}` result for this fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_results(files=results_files):\n",
    "    return pd.concat([pd.read_csv(file) for file in files], ignore_index=True)\n",
    "\n",
    "def create_file(*path_tokens):\n",
    "    path = os.path.realpath(os.path.join(*path_tokens))\n",
    "    if not os.path.exists(path):\n",
    "        dirname, basename = os.path.split(path)\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname, exist_ok=True)\n",
    "        if basename:\n",
    "            open(path, 'a').close()\n",
    "    return path\n",
    "\n",
    "def display(fr, pretty=True, float_format=ff):\n",
    "    with pd.option_context(\n",
    "        'display.max_rows', len(fr), \n",
    "        'display.float_format', lambda f: float_format % f\n",
    "        ):\n",
    "        if type(fr) is pd.Series:\n",
    "            fr = fr.to_frame()\n",
    "        if pretty and type(fr) is pd.DataFrame:\n",
    "            fr.style.set_properties(**{'vertical-align':'top'})\n",
    "            idisplay.display(idisplay.HTML(fr.to_html()))\n",
    "        else:\n",
    "            print(fr)\n",
    "        \n",
    "        \n",
    "def build_classification_type_map(results_df):\n",
    "    cp = results_df.loc[(results_df.framework=='constantpredictor')&(results_df.fold==0)]\n",
    "#     binary_tasks = cp.where(pd.notna(cp.auc))['task'].dropna().tolist()\n",
    "    return (cp.apply(lambda r: pd.Series([r.task, 'binary' if not np.isnan(r.auc) else 'multiclass'], \n",
    "                                         index=['task', 'type']), \n",
    "                     axis=1,\n",
    "                     result_type='expand')\n",
    "              .set_index('task')['type']\n",
    "              .to_dict())    \n",
    "    \n",
    "\n",
    "def classification_type(row, type_map):\n",
    "    return type_map.get(row.task)\n",
    "\n",
    "\n",
    "def impute_result(row, results_df, res_col='result', ref_framework=impute_missing_with):\n",
    "    if pd.notna(row[res_col]):\n",
    "        return row[res_col]\n",
    "    # if all folds are failed or missing, don't impute\n",
    "    if pd.isna(results_df.loc[(results_df.task==row.task)&(results_df.framework==row.framework)][res_col]).all():\n",
    "        return np.nan\n",
    "    # impute with ref framework corresponding value\n",
    "    return (results_df.loc[(results_df.framework==ref_framework)\n",
    "                           &(results_df.task==row.task)\n",
    "                           &(results_df.fold==row.fold)][res_col]\n",
    "                     .item())\n",
    "\n",
    "\n",
    "def imputed(row):\n",
    "    return pd.isna(row.result) and pd.notna(row.imp_result)\n",
    "    \n",
    "\n",
    "def score(row, res_col='result'):\n",
    "    return row[res_col] if row[res_col] in [row.auc, row.acc]\\\n",
    "                        else - row[res_col]\n",
    "\n",
    "\n",
    "def norm_score(row, results_df, score_col='score', zero_one_refs=zero_one_refs):\n",
    "    zero, one = (results_df.loc[(results_df.framework==ref)\n",
    "                                &(results_df.task==row.task)\n",
    "                                &(results_df.fold==row.fold)][score_col]\n",
    "                           .item()\n",
    "                 for ref in zero_one_refs)\n",
    "    return (row[score_col] - zero) / (one - zero)\n",
    " \n",
    "    \n",
    "def sorted_ints(arr): \n",
    "    return sorted(list(map(int, arr[~np.isnan(arr)])))\n",
    "\n",
    "all_results = load_results().replace(renamings)\n",
    "all_results = all_results.loc[~all_results.framework.isin(excluded_frameworks)]\n",
    "all_results.task = all_results.task.str.lower()\n",
    "all_results.framework = all_results.framework.str.lower()\n",
    "all_results.fold = all_results.fold.apply(int)\n",
    "\n",
    "all_frameworks = all_results.framework.unique()\n",
    "all_frameworks.sort()\n",
    "all_tasks = all_results.task.unique()\n",
    "all_tasks.sort()\n",
    "all_folds = all_results.fold.unique()\n",
    "class_type_map = build_classification_type_map(all_results)\n",
    "\n",
    "\n",
    "all_done = all_results.set_index(['task', 'fold', 'framework'])\n",
    "if not all_done.index.is_unique:\n",
    "    print(\"Duplicate entries:\")\n",
    "    display(all_done[all_done.index.duplicated(keep=False)].sort_values(by=all_done.index.names), \n",
    "            pretty=False)\n",
    "assert all_done.index.is_unique\n",
    "all_missing = pd.DataFrame([(task, fold, framework, 'missing') \n",
    "                            for task in all_tasks \n",
    "                            for fold in range(nfolds)\n",
    "                            for framework in all_frameworks \n",
    "                            if (task, fold, framework) not in all_done.index],\n",
    "                           columns=[*all_done.index.names, 'info'])\\\n",
    "                          .set_index(all_done.index.names)\n",
    "assert all_missing.index.is_unique\n",
    "all_failed = all_results.loc[pd.notna(all_results['info'])]\\\n",
    "                        .set_index(all_done.index.names)\n",
    "assert all_failed.index.is_unique\n",
    "\n",
    "# extending the data frame \n",
    "all_results = all_results.append(all_missing.reset_index())\n",
    "all_results['type'] = [classification_type(row, class_type_map) for _, row in all_results.iterrows()]\n",
    "all_results['score'] = [score(row) for _, row in all_results.iterrows()]\n",
    "\n",
    "all_results['imp_result'] = [impute_result(row, all_results) for _, row in all_results.iterrows()]\n",
    "all_results['imp_score'] = [impute_result(row, all_results, 'score') for _, row in all_results.iterrows()]\n",
    "all_results['norm_score'] = [norm_score(row, all_results, 'imp_score') for _, row in all_results.iterrows()]\n",
    "\n",
    "all_results.to_csv(create_file(\"tables\", results_group, \"all_results.csv\"), \n",
    "                   index=False, \n",
    "                   float_format=ff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tasks = (all_results.groupby(['task', 'type'])['id']\n",
    "                    .unique()\n",
    "                    .map(lambda id: id[0]))\n",
    "display(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completed tasks/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "done = (all_done.reset_index()\n",
    "                .groupby(['task', 'framework'])['fold']\n",
    "                .unique())\n",
    "display(done, pretty=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing or crashed/aborted tasks/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# not_done = pd.DataFrame([(task, framework) for task in all_tasks \n",
    "#                                            for framework in all_frameworks \n",
    "#                                            if (task, framework) not in done.index],\n",
    "#                         columns=['task', 'framework'])\n",
    "# missing = all_results.append(not_done)\\\n",
    "#                      .groupby(['task', 'framework'])['fold']\\\n",
    "#                      .unique()\\\n",
    "#                      .map(sorted_ints)\\\n",
    "#                      .map(lambda arr: sorted(list(set(range(0, nfolds)) - set(arr))))\\\n",
    "#                      .where(lambda values: values.map(lambda arr: len(arr) > 0))\\\n",
    "#                      .dropna()\n",
    "\n",
    "missing = (all_missing.reset_index()\n",
    "                      .groupby(['task', 'framework'])['fold']\n",
    "                      .unique())\n",
    "display(missing, pretty=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failing tasks/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# failed = all_results.where(np.isnan(all_results.result))\\\n",
    "#                     .groupby(['task', 'framework'])['fold']\\\n",
    "#                     .unique()\\\n",
    "#                     .map(sorted_ints)\n",
    "\n",
    "failed = (all_failed.reset_index()\n",
    "                    .groupby(['task', 'framework'])['fold']\n",
    "                    .unique())\n",
    "display(failed, pretty=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def list_outliers(col, results=all_results, z_threshold=3):\n",
    "    df = results.pivot_table(index=['type','task', 'framework'], columns='fold', values=col)\n",
    "    df_mean = df.mean(axis=1)\n",
    "    df_std = df.std(axis=1)\n",
    "    z_score = (df.sub(df_mean, axis=0)\n",
    "                 .div(df_std, axis=0)\n",
    "                 .abs())\n",
    "    return z_score.where(z_score > z_threshold).dropna(axis=0, how='all')\n",
    "    \n",
    "display(list_outliers('result', \n",
    "                      z_threshold=2.5,\n",
    "#                       results=all_results.loc[all_results.framework=='h2oautoml']\n",
    "                     ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging using arithmetic mean over fold `result` or `score`.\n",
    "In following summaries, if not mentioned otherwise, the means are computed over imputed results/scores.\n",
    "Given a task and a framework:\n",
    "- if **all folds results/scores are missing**, then no imputation occured, and the mean result is `nan`.\n",
    "- if **only some folds results/scores are missing**, then the amount of imputed results that contributed to the mean are displayed between parenthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_imputed_mark(values, imp, val_type=float, val_format=None):\n",
    "    formats = dict(float=\"{:,.6g}{}\", int=\"{0:d}{}\", str=\"{}{}\")\n",
    "    format_value = (val_format if val_format is not None\n",
    "                               else lambda *val: formats[val_type.__name__].format(*val))\n",
    "    return (values.astype(object)\n",
    "                  .combine(imp, \n",
    "                           lambda val, imp: format_value(val, \" ({:.0g})\".format(imp) if imp else '')))\n",
    "\n",
    "def render_summary(col, results=all_results, show_imputations=True, filename=None, float_format=ff):\n",
    "    res_group = results.groupby(['type', 'task', 'framework'])\n",
    "    df = res_group[col].mean().unstack()\n",
    "    if show_imputations:\n",
    "        imputed_df = (res_group['result', 'imp_result']\n",
    "                          .apply(lambda df: sum(imputed(row) for _, row in df.iterrows()))\n",
    "                          .unstack())    \n",
    "        df = df.combine(imputed_df, ft.partial(add_imputed_mark, \n",
    "                                               val_format=lambda *v: (float_format+\"%s\") % tuple(v)))\n",
    "    display(df, float_format=float_format)\n",
    "    if filename is not None:\n",
    "        df.to_csv(create_file(\"tables\", results_group, filename), float_format=float_format)\n",
    "\n",
    "\n",
    "summary_results = all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of models trained\n",
    "\n",
    "When available, displays the average amount of models trained by the framework for each dataset.\n",
    "\n",
    "This amount should be interpreted differently for each framework.\n",
    "For example, with *RandomForest*, this amount corresponds to the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_summary('models', \n",
    "               results=summary_results, \n",
    "               filename=\"models_summary.csv\", \n",
    "               float_format=\"%.f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "render_summary('result', \n",
    "               results=summary_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_summary('imp_result', \n",
    "               results=summary_results,\n",
    "               filename=\"result_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_summary('imp_score', \n",
    "               results=summary_results,\n",
    "               filename=\"score_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_summary('norm_score', \n",
    "               results=summary_results,\n",
    "               filename=\"norm_score_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def rank(scores):\n",
    "    sorted_scores = pd.Series(scores.unique()).sort_values(ascending=False)\n",
    "    ranks = pd.Series(index=scores.index)\n",
    "    for idx, value in scores.items():\n",
    "        try:\n",
    "            ranks.at[idx] = np.where(sorted_scores == value)[0][0]+1\n",
    "        except IndexError:\n",
    "            ranks.at[idx] = np.nan\n",
    "    return ranks\n",
    "\n",
    "def render_leaderboard(col, results=all_results, aggregate=False, show_imputations=False, filename=None):\n",
    "    res_group = results.groupby(['type', 'task', 'framework'])\n",
    "    df = (res_group[col].mean().unstack() if aggregate \n",
    "          else results.pivot_table(index=['type','task', 'fold'], columns='framework', values=col))\n",
    "    df = (df.apply(rank, axis=1, result_type='broadcast')\n",
    "            .astype(object)) \n",
    "    if show_imputations:\n",
    "        imputed_df = (res_group['result', 'imp_result']\n",
    "                          .apply(lambda df: sum(imputed(row) for _, row in df.iterrows()))\n",
    "                          .unstack())    \n",
    "        df = df.combine(imputed_df, add_imputed_mark)\n",
    "    display(df)\n",
    "    if filename is not None:\n",
    "        df.to_csv(create_file(\"tables\", results_group, filename), float_format='%.f')\n",
    "     \n",
    "    \n",
    "leaderboard_results = all_results.loc[~all_results.framework.isin(['constantpredictor', 'randomforest'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "render_leaderboard('imp_score', \n",
    "                   results=leaderboard_results,\n",
    "                   aggregate=True, \n",
    "                   show_imputations=True, \n",
    "                   filename=\"tasks_leaderboard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folds leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "render_leaderboard('score', filename=\"folds_leaderboard.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def savefig(fig, path):\n",
    "    fig.savefig(path, bbox_inches='tight')\n",
    "    \n",
    "\n",
    "def task_labels(index):\n",
    "    max_length = 16\n",
    "    return (index.droplevel('type')\n",
    "                .map(lambda x: x if len(x) <= max_length else u'{}…'.format(x[:max_length-1]))\n",
    "                .values)\n",
    "\n",
    "def set_labels(axes, \n",
    "               title=None,\n",
    "               xlabel=None, ylabel=None,\n",
    "               x_labels=None, y_labels=None, \n",
    "               legend_title=None):\n",
    "    axes.set_title(title, fontsize='xx-large')\n",
    "    axes.set_xlabel(xlabel, fontsize='x-large')\n",
    "    axes.set_ylabel(ylabel, fontsize='x-large')\n",
    "    axes.tick_params(labelsize='x-large')\n",
    "    if x_labels is not None:\n",
    "        axes.set_xticklabels(x_labels)\n",
    "    if y_labels is not None:\n",
    "        axes.set_yticklabels(y_labels)\n",
    "    legend = axes.get_legend()\n",
    "    if legend is not None:\n",
    "        legend_title = legend_title or legend.get_title().get_text()\n",
    "        legend.set_title(legend_title, prop=dict(size='x-large'))\n",
    "        for text in legend.get_texts():\n",
    "            text.set_fontsize('x-large')\n",
    "            \n",
    "def set_scales(axes, xscale=None, yscale=None):\n",
    "    if isinstance(xscale, str):\n",
    "        axes.set_xscale(xscale)\n",
    "    elif isinstance(xscale, tuple):\n",
    "        axes.set_xscale(xscale[0], **xscale[1])\n",
    "    if isinstance(yscale, str):\n",
    "        axes.set_yscale(yscale)\n",
    "    elif isinstance(yscale, tuple):\n",
    "        axes.set_yscale(yscale[0], **yscale[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_heatmap(df, \n",
    "                 x_labels=True, y_labels=True, \n",
    "                 title=None, xlabel=None, ylabel=None,\n",
    "                 **kwargs):\n",
    "    with sb.axes_style('white'), sb.plotting_context('paper'):\n",
    "#         print(sb.axes_style())\n",
    "#         print(sb.plotting_context())\n",
    "        axes = sb.heatmap(df, xticklabels=x_labels, yticklabels=y_labels,\n",
    "                          annot=True, cmap='RdYlGn', robust=True,\n",
    "                          **kwargs)\n",
    "        axes.tick_params(axis='y', labelrotation=0) \n",
    "        set_labels(axes, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "        fig = axes.get_figure()\n",
    "        fig.set_size_inches(10, df.shape[0]/2)\n",
    "        fig.set_dpi(120)\n",
    "        return fig\n",
    "\n",
    "def draw_score_heatmap(col, results=all_results, type_filter='all', filename=None, **kwargs):\n",
    "    df = (results.groupby(['type', 'task', 'framework'])[col]\n",
    "                 .mean()\n",
    "                 .unstack())\n",
    "    df = (df if type_filter == 'all'\n",
    "             else df[df.index.get_loc(type_filter)])\n",
    "    fig = draw_heatmap(df, \n",
    "                       y_labels=task_labels(df.index), \n",
    "#                        xlabel=\"Framework\", ylabel=\"Task\",\n",
    "                       **kwargs)\n",
    "    if filename is not None:\n",
    "        savefig(fig, create_file(\"graphics\", results_group, filename))\n",
    "    return fig\n",
    "\n",
    "\n",
    "# heatmap_results = all_results.loc[~all_results.framework.isin(['constantpredictor', 'randomforest'])]\n",
    "heatmap_results = all_results.loc[~all_results.framework.isin(['constantpredictor'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_heatmap('imp_score',\n",
    "                   results=heatmap_results,\n",
    "                   type_filter='binary', \n",
    "                   title=f\"Scores ({binary_score_label}) on {results_group} binary classification problems\",\n",
    "                   filename=\"binary_score_heat.png\",\n",
    "                   center=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_heatmap('imp_score', \n",
    "                   results=heatmap_results,\n",
    "                   type_filter='multiclass', \n",
    "                   title=f\"Scores ({multiclass_score_label}) on {results_group} multi-class classification problems\",\n",
    "                   filename=\"multiclass_score_heat.png\",\n",
    "                   center=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_heatmap('norm_score', \n",
    "                   results=heatmap_results,\n",
    "                   type_filter='binary', \n",
    "                   title=f\"Normalized scores on {results_group} binary classification problems\",\n",
    "                   filename=\"binary_norm_score_heat.png\",\n",
    "                   center=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_heatmap('norm_score',\n",
    "                   results=heatmap_results,\n",
    "                   type_filter='multiclass', \n",
    "                   title=f\"Normalized scores on {results_group} multi-class classification problems\",\n",
    "                   filename=\"multiclass_norm_score_heat.png\",\n",
    "                   center=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_parallel_coord(df, class_column, \n",
    "                        x_labels=True, yscale='linear', \n",
    "                        title=None, xlabel=None, ylabel=None,\n",
    "                        legend_loc='best', legend_title=None, colormap=colormap):\n",
    "    with sb.axes_style('ticks', rc={'grid.linestyle': 'dotted'}), sb.plotting_context('paper'):\n",
    "#         print(sb.axes_style())\n",
    "        parallel_fig = mp.pyplot.figure(dpi=120, figsize=(10, df.shape[0]))\n",
    "        # select the first colors from the colormap to ensure we use the same colors as in the stripplot later\n",
    "        colors = mp.cm.get_cmap(colormap).colors[:len(df[class_column].unique())]\n",
    "        axes = pd.plotting.parallel_coordinates(df, \n",
    "                                                class_column=class_column, \n",
    "                                                colors=colors,\n",
    "                                                axvlines=False,\n",
    "                                               )\n",
    "        axes.tick_params(axis='x', labelrotation=90) \n",
    "        set_scales(axes, yscale=yscale)\n",
    "        handles, labels = axes.get_legend_handles_labels()\n",
    "        axes.legend(handles, labels, loc=legend_loc, title=legend_title)\n",
    "        set_labels(axes, title=title, xlabel=xlabel, ylabel=ylabel, x_labels=x_labels)\n",
    "        return parallel_fig\n",
    "\n",
    "\n",
    "def draw_score_parallel_coord(col, results=all_results, type_filter='all', \n",
    "                              ylabel=None, filename=None, **kwargs):\n",
    "    res_group = results.groupby(['type', 'task', 'framework'])\n",
    "    df = res_group[col].mean().unstack(['type', 'task'])\n",
    "    df = df if type_filter == 'all' \\\n",
    "            else df.iloc[:, df.columns.get_loc(type_filter)]\n",
    "    df.reset_index(inplace=True)\n",
    "    fig = draw_parallel_coord(df, \n",
    "                              'framework',\n",
    "                              x_labels=task_labels(df.columns.drop('framework')),\n",
    "#                               xlabel=\"Task\",\n",
    "                              ylabel=ylabel or \"Score\",\n",
    "                              legend_title=\"Framework\",\n",
    "                              **kwargs) \n",
    "    if filename is not None:\n",
    "        savefig(fig, create_file(\"graphics\", results_group, filename))\n",
    "    return fig\n",
    "\n",
    "\n",
    "# parallel_coord_results = all_results.loc[~all_results.framework.isin(['randomforest'])]\n",
    "parallel_coord_results = all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_parallel_coord('imp_score',\n",
    "                          results=parallel_coord_results,\n",
    "                          type_filter='binary', \n",
    "                          title=f\"Scores ({binary_score_label}) on {results_group} binary classification problems\",\n",
    "                          ylabel=binary_score_label,\n",
    "                          legend_loc='lower left',\n",
    "                          filename=\"binary_score_parallel_ccord.png\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_parallel_coord('imp_score',\n",
    "                          results=parallel_coord_results,\n",
    "                          type_filter='multiclass',\n",
    "                          title=f\"Scores ({multiclass_score_label}) on {results_group} multi-class classification problems\",\n",
    "                          ylabel=multiclass_score_label,\n",
    "                          yscale=('symlog', dict(linthreshy=0.5)),\n",
    "                          legend_loc='lower left',\n",
    "                          filename=\"multiclass_score_parallel_ccord.png\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_parallel_coord('norm_score', \n",
    "                          results=parallel_coord_results,\n",
    "                          type_filter='binary', \n",
    "                          title=f\"Normalized scores on {results_group} binary classification problems\",\n",
    "                          filename=\"binary_norm_score_parallel_ccord.png\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_parallel_coord('norm_score', \n",
    "                          results=parallel_coord_results,\n",
    "                          type_filter='multiclass',\n",
    "                          title=f\"Normalized scores on {results_group} multi-class classification problems\",\n",
    "                          filename=\"multiclass_norm_score_parallel_ccord.png\", \n",
    "                          yscale='symlog',\n",
    "                         );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_stripplot(df, x, y, hue, \n",
    "                   xscale='linear', xbound=None, \n",
    "                   xlabel=None, ylabel=None, y_labels=None, title=None,\n",
    "                   legend_title=None, legend_loc='best', colormap=colormap):\n",
    "    with sb.axes_style('whitegrid', rc={'grid.linestyle': 'dotted'}), sb.plotting_context('paper'):\n",
    "#         print(sb.axes_style())\n",
    "        # Initialize the figure\n",
    "        strip_fig, axes = mp.pyplot.subplots(dpi=120, figsize=(10, len(df.index.unique())))\n",
    "        set_scales(axes, xscale=xscale)\n",
    "        if xbound is not None:   \n",
    "            axes.set_autoscalex_on(False)\n",
    "            axes.set_xbound(*xbound)\n",
    "#             axes.invert_xaxis()\n",
    "        sb.despine(bottom=True, left=True)\n",
    "\n",
    "        # Show each observation with a scatterplot\n",
    "        sb.stripplot(x=x, y=y, hue=hue,\n",
    "                     data=df, dodge=True, jitter=True, palette=colormap,\n",
    "                     alpha=.25, zorder=1)\n",
    "\n",
    "        # Show the conditional means\n",
    "        sb.pointplot(x=x, y=y, hue=hue,\n",
    "                     data=df, dodge=.5, join=False, palette=colormap,\n",
    "                     markers='d', scale=.75, ci=None)\n",
    "\n",
    "        # Improve the legend \n",
    "        handles, labels = axes.get_legend_handles_labels()\n",
    "        dist = int(len(labels)/2)\n",
    "        axes.legend(handles[dist:], labels[dist:], title=legend_title or hue,\n",
    "                    handletextpad=0, columnspacing=1,\n",
    "                    loc=legend_loc, ncol=1, frameon=True)\n",
    "        set_labels(axes, title=title, xlabel=xlabel, ylabel=ylabel, y_labels=y_labels)\n",
    "        return strip_fig\n",
    "\n",
    "\n",
    "def draw_score_stripplot(col, results=all_results, type_filter='all', filename=None, **kwargs):\n",
    "    scatterplot_df = results.set_index(['type', 'task']).sort_index()\n",
    "    df = scatterplot_df if type_filter == 'all' \\\n",
    "                        else scatterplot_df[scatterplot_df.index.get_loc(type_filter)]\n",
    "    fig = draw_stripplot(\n",
    "        df,\n",
    "        x=col,\n",
    "        y=df.index,\n",
    "        hue='framework',\n",
    "#         ylabel='Task',\n",
    "        y_labels=task_labels(df.index.unique()),\n",
    "        legend_title=\"Framework\",\n",
    "        **kwargs\n",
    "    )\n",
    "    if filename is not None:\n",
    "        savefig(fig, create_file(\"graphics\", results_group, filename))\n",
    "    return fig\n",
    "\n",
    "\n",
    "# scatterplot_results = (all_results.loc[~all_results.framework.isin(['randomforest'])]\n",
    "#                                   .sort_values(by=['framework']))  # sorting for colors consistency\n",
    "scatterplot_results = all_results.sort_values(by=['framework'])  # sorting for colors consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_stripplot('imp_result', \n",
    "                     results=scatterplot_results,\n",
    "                     type_filter='binary', \n",
    "                     title=f\"Scores on {results_group} binary classification problems\",\n",
    "                     xlabel=binary_score_label,\n",
    "                     filename=\"binary_results_stripplot.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_stripplot('imp_result',\n",
    "                     results=scatterplot_results,\n",
    "                     type_filter='multiclass', \n",
    "#                      xbound=(0,10),\n",
    "                     xscale=('symlog', dict(linthreshx=0.5)),\n",
    "                     title=f\"Scores on {results_group} multi-class classification problems\",\n",
    "                     xlabel=multiclass_score_label, \n",
    "                     filename=\"multiclass_results_stripplot.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_score_stripplot('norm_score', \n",
    "                     results=scatterplot_results,\n",
    "                     type_filter='binary', \n",
    "                     xbound=(-0.2, 2),\n",
    "                     xscale='linear',\n",
    "                     title=f\"Normalized scores on {results_group} binary classification problems\",\n",
    "                     filename=\"binary_norm_score_stripplot.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "draw_score_stripplot('norm_score', \n",
    "                     results=scatterplot_results,\n",
    "                     type_filter='multiclass', \n",
    "                     xbound=(-0.2, 2.5),\n",
    "                     xscale='linear',\n",
    "                     title=f\"Normalized scores on {results_group} multi-class classification problems\",\n",
    "                     filename=\"multiclass_norm_score_stripplot.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_results.loc[(all_results.task.str.contains('jungle'))&(all_results.framework=='tunedrandomforest')];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "done.iloc[done.index.get_level_values('framework').isin(['autosklearn', 'h2oautoml', 'tpot'])]\\\n",
    "    .apply(sorted_ints);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "failures = all_failed.groupby(['task', 'fold', 'framework'])['info']\\\n",
    "                     .unique()\n",
    "#display(failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
